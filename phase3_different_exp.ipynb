{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This notebook is used to analyse the best combinis in detail. All the tests with length, noise etc. are being analysed here.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import iqr\n",
    "\n",
    "from src.main.rl.evaluation.plots.phase3_plots import (\n",
    "    plot_actions_taken,\n",
    "    plot_observations,\n",
    ")\n",
    "from src.main.rl.evaluation.phase3_evaluation import create_evaluation_df_phase3\n",
    "from src.main.rl.utils.constants import color_mapping\n",
    "\n",
    "paths = [\n",
    "    \"src/main/rl/models/scenario1/training_04_06/scenario1_ActionSpaceOption3Wrapper_ObservationOption4Wrapper_None_RewardOption2Wrapper_TD3_training_04_06\",\n",
    "    \"src/main/rl/models/scenario1/training_04_06/scenario1_ActionSpaceOption3Wrapper_ObservationOption5Wrapper_NPPAutomationWrapper_RewardOption2Wrapper_SAC_training_04_06\",\n",
    "    \"src/main/rl/models/scenario2/training_04_06/scenario2_ActionSpaceOption3Wrapper_ObservationOption5Wrapper_None_RewardOption2Wrapper_PPO_training_04_06\",\n",
    "    \"src/main/rl/models/scenario2/training_04_06/scenario2_ActionSpaceOption3Wrapper_ObservationOption4Wrapper_NPPAutomationWrapper_RewardOption2Wrapper_PPO_training_04_06\",\n",
    "    \"src/main/rl/models/scenario3/training_04_06/scenario3_ActionSpaceOption3Wrapper_ObservationOption5Wrapper_None_RewardOption2Wrapper_PPO_training_04_06\",\n",
    "    \"src/main/rl/models/scenario3/training_04_06/scenario3_ActionSpaceOption3Wrapper_ObservationOption3Wrapper_NPPAutomationWrapper_RewardOption2Wrapper_PPO_training_04_06\",\n",
    "]\n",
    "path_to_save = \"src/main/rl/evaluation/output/phase3_evaluation_results.csv\"\n",
    "pd.options.display.max_colwidth = 500\n",
    "df = pd.DataFrame()\n",
    "try:\n",
    "    df = pd.read_csv(path_to_save)\n",
    "except:\n",
    "    pass\n",
    "if df.empty:\n",
    "    create_evaluation_df_phase3(path_to_save, paths)\n",
    "    df = pd.read_csv(path_to_save)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics per combination\n",
    "df_statistics_per_combination = (\n",
    "    df.drop(columns=[\"full_path\"])\n",
    "    .groupby(\n",
    "        [\n",
    "            \"combination\",\n",
    "            \"scenario\",\n",
    "            \"alg\",\n",
    "            \"action_wrapper\",\n",
    "            \"obs_wrapper\",\n",
    "            \"automation_wrapper\",\n",
    "        ],\n",
    "        dropna=False,\n",
    "    )\n",
    "    .agg([\"mean\", \"max\", \"min\", \"std\", iqr])\n",
    ")\n",
    "# Necessary to set index as alphabetical is confusing for the thesis\n",
    "df_statistics_per_combination[\"index\"] = [0, 1, 3, 2, 5, 4]\n",
    "df_statistics_per_combination = (\n",
    "    df_statistics_per_combination.reset_index()\n",
    "    .set_index([\"index\", \"combination\"])\n",
    "    .sort_index()\n",
    ")\n",
    "df_statistics_per_combination.columns = [\n",
    "    \"_\".join(a) for a in df_statistics_per_combination.columns.to_flat_index()\n",
    "]\n",
    "save_df_per_combination = df_statistics_per_combination.copy()\n",
    "df_statistics_per_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOP Analysis\n",
    "from src.main.rl.evaluation.eval import evaluate_sop\n",
    "\n",
    "(\n",
    "    cum_reward_sop,\n",
    "    criticality_score_sop,\n",
    "    total_timesteps_sop,\n",
    "    actions_taken_sop,\n",
    "    obs_taken_sop,\n",
    "    info_sop,\n",
    ") = evaluate_sop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main.rl.evaluation.plots.phase3_plots import (\n",
    "    prepare_one_combination_actions_and_obs_for_analysis,\n",
    ")\n",
    "from src.main.rl.utils.constants import color_mapping\n",
    "from src.main.rl.utils.constants import (\n",
    "    scaling_factors_scenario_1,\n",
    "    action_dimensions_german,\n",
    "    obs_scaling_factors,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = \"src/main/rl/models/scenario1/training_04_06/scenario1_ActionSpaceOption3Wrapper_ObservationOption4Wrapper_None_RewardOption2Wrapper_TD3_training_04_06\"\n",
    "\n",
    "(\n",
    "    actions_prepared,\n",
    "    obs_prepared,\n",
    "    list_of_all_actions_taken,\n",
    "    list_of_all_obs_taken,\n",
    ") = prepare_one_combination_actions_and_obs_for_analysis(path, obs_dimensions=6)\n",
    "\n",
    "for idx, item in enumerate(actions_prepared):\n",
    "    # actions are scaled in the models, so we need to rescale\n",
    "    item = item.applymap(\n",
    "        lambda x: int(round((x + 1) * (scaling_factors_scenario_1[idx] / 2)))\n",
    "    )\n",
    "    # get min, mean, max for plotting\n",
    "    actions_prepared[idx] = item.agg([\"mean\"], axis=1)\n",
    "\n",
    "for idx, item in enumerate(obs_prepared):\n",
    "    item = item.fillna(0)\n",
    "    # rescaling obs\n",
    "    item = item.applymap(\n",
    "        lambda x: int(round((x + 1) * (obs_scaling_factors[6][idx] / 2)))\n",
    "    )\n",
    "    obs_prepared[idx] = item.agg([\"mean\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_axis_scale_actions = [[0, 100], [0, 2200], [0, 1.1], [0, 1.1], [0, 2200]]\n",
    "\n",
    "fig, ax = plot_actions_taken(\n",
    "    actions_taken_sop, \"scenario1\", y_axis_scale_actions, color=\"standard\"\n",
    ")\n",
    "[\n",
    "    ax[idx].plot(item[\"mean\"], color=color_mapping[\"grey\"])\n",
    "    for idx, item in enumerate(actions_prepared)\n",
    "]\n",
    "\n",
    "fig.savefig(\n",
    "    f\"src/main/rl/evaluation/plot_results/phase3_action_sop_combi1.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_axis_scale_obs = [[0, 1000], [0, 4000], [0, 550], [0, 8000], [0, 180], [0, 32]]\n",
    "fig, ax = plot_observations(obs_taken_sop, y_axis_scale_obs)\n",
    "[\n",
    "    ax[idx].plot(item[\"mean\"], color=color_mapping[\"grey\"])\n",
    "    for idx, item in enumerate(obs_prepared)\n",
    "]\n",
    "fig.savefig(\n",
    "    f\"src/main/rl/evaluation/plot_results/phase3_obs_sop_combi1.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main.rl.utils.constants import (\n",
    "    scaling_factors_scenario_1,\n",
    "    scaling_factors_scenario_2,\n",
    "    action_dimensions_german,\n",
    "    obs_scaling_factors,\n",
    "    obs_dimensions_german,\n",
    "    scaling_factors_scenario_3,\n",
    ")\n",
    "obs_positions = [[] for i in range(6)]\n",
    "current_obs_scaling_factors = obs_scaling_factors[6]\n",
    "for item in obs_taken_sop:\n",
    "    [obs_positions[idx].append(single_action) for idx, single_action in enumerate(item)]\n",
    "for idx, item in enumerate(obs_positions):\n",
    "        scaled_values_obs.append([int(round((x + 1) * (current_obs_scaling_factors[idx] / 2))) for x in item])\n",
    "scaled_values_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 length\n",
    "print(df[\"episode_length_1000_timesteps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of succesfull models doing 1000 steps per combination.\n",
    "length_1000 = (\n",
    "    df[[\"episode_length_1000_timesteps\", \"combination\"]]\n",
    "    .set_index(\"combination\")\n",
    "    .eq(1000)\n",
    "    .groupby(\"combination\")\n",
    "    .sum()\n",
    "    .transpose()\n",
    "    .sum()\n",
    "    .transpose()\n",
    "    .to_latex()\n",
    ")\n",
    "print(length_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how often the pumps are blown in the 1000 length test\n",
    "df[\n",
    "    [\n",
    "        \"episode_length_1000_condensator_pump_blown\",\n",
    "        \"episode_length_1000_water_pump_blown\",\n",
    "    ]\n",
    "].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main.rl.evaluation.eval import evaluate_terminal_state_obs\n",
    "from src.main.rl.utils.combined_parser import parse_information_from_path\n",
    "\n",
    "path = \"src/main/rl/models/scenario3/training_04_06/scenario3_ActionSpaceOption3Wrapper_ObservationOption3Wrapper_NPPAutomationWrapper_RewardOption2Wrapper_PPO_training_04_06_10/best_model.zip\"\n",
    "scenario, alg, wrapper_maker = parse_information_from_path(path)\n",
    "cum_reward, obs, info = evaluate_terminal_state_obs(\n",
    "    scenario, path, alg, wrapper_maker, episode_length=1000\n",
    ")\n",
    "# normalized_power_output = 2 * (in\n",
    "# normalized_reactor_water_level =\n",
    "# normalized_reactor_pressure = 2 *\n",
    "# normalized_condenser_water_level\n",
    "# normalized_condenser_pressure = 2\n",
    "scaled_values_obs = []\n",
    "obs_scaling_factors = [800, 4000, 550, 8000, 180, 30, 0]\n",
    "for l_idx, item in enumerate(obs):\n",
    "    scaled_values_obs.append(\n",
    "        [\n",
    "            int(round((x + 1) * (obs_scaling_factors[idx] / 2)))\n",
    "            for idx, x in enumerate(item)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criticality Score in more detail\n",
    "cols = [item for item in df_statistics_per_combination.columns if \"criti\" in item]\n",
    "df_statistics_per_combination[cols].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise\n",
    "cols = [item for item in df_statistics_per_combination.columns if \"DelayNoise\" in item]\n",
    "df_statistics_per_combination[cols].round(2)\n",
    "\n",
    "print(df_statistics_per_combination[[\"DelayNoiseWrapperOption1_mean\"]].round(2))\n",
    "print(\n",
    "    df_statistics_per_combination[cols]\n",
    "    .round(2)\n",
    "    .droplevel(1)[\n",
    "        [\n",
    "            \"DelayNoiseWrapperOption1_mean\",\n",
    "            \"DelayNoiseWrapperOption1_max\",\n",
    "            \"DelayNoiseWrapperOption1_min\",\n",
    "            \"DelayNoiseWrapperOption1_std\",\n",
    "            \"DelayNoiseWrapperOption1_iqr\",\n",
    "            \"DelayNoiseWrapperOption2_mean\",\n",
    "            \"DelayNoiseWrapperOption2_max\",\n",
    "            \"DelayNoiseWrapperOption2_min\",\n",
    "            \"DelayNoiseWrapperOption2_std\",\n",
    "            \"DelayNoiseWrapperOption2_iqr\",\n",
    "        ]\n",
    "    ]\n",
    "    .transpose()\n",
    "    .to_latex()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plots for noise per option and per combiniation using timesteps\n",
    "def set_box_color(bp, color, idx):\n",
    "    plt.setp(bp[\"boxes\"], color=color)\n",
    "    plt.setp(bp[\"whiskers\"], color=color)\n",
    "    plt.setp(bp[\"caps\"], color=color)\n",
    "    plt.setp(bp[\"medians\"], color=color)\n",
    "    plt.setp(bp[\"caps\"], color=color)\n",
    "    plt.setp(bp[\"fliers\"], color=color)\n",
    "\n",
    "\n",
    "noise_df = df[\n",
    "    [\n",
    "        \"obs_wrapper\",\n",
    "        \"automation_wrapper\",\n",
    "        \"scenario\",\n",
    "        \"DelayNoiseWrapperOption2_timesteps\",\n",
    "        \"DelayNoiseWrapperOption1_timesteps\",\n",
    "        \"combination\",\n",
    "        \"ObservationVariesNoiseWrapper1_timesteps\",\n",
    "        \"ObservationVariesPositiveNoiseWrapper_timesteps\",\n",
    "        \"ObservationVariesNegativeNoiseWrapper_timesteps\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "order_of_col = [\n",
    "    \"ObservationVariesPositiveNoiseWrapper_timesteps\",\n",
    "    \"ObservationVariesNegativeNoiseWrapper_timesteps\",\n",
    "    \"ObservationVariesNoiseWrapper1_timesteps\",\n",
    "    \"DelayNoiseWrapperOption1_timesteps\",\n",
    "    \"DelayNoiseWrapperOption2_timesteps\",\n",
    "]\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "fig.set_figwidth(14)\n",
    "\n",
    "ax1 = ax.boxplot(\n",
    "    noise_df.query(\n",
    "        \"combination=='src/main/rl/models/scenario1/training_04_06/scenario1_ActionSpaceOption3Wrapper_ObservationOption4Wrapper_None_RewardOption2Wrapper_TD3_training_04_06'\"\n",
    "    )[order_of_col],\n",
    "    positions=[0, 0.2, 0.4, 0.6, 0.8],\n",
    "    labels=[\"1a\", \"1b\", \"1c\", \"2a\", \"2b\"],\n",
    ")\n",
    "ax2 = ax.boxplot(\n",
    "    noise_df.query(\n",
    "        \"combination=='src/main/rl/models/scenario1/training_04_06/scenario1_ActionSpaceOption3Wrapper_ObservationOption5Wrapper_NPPAutomationWrapper_RewardOption2Wrapper_SAC_training_04_06'\"\n",
    "    )[order_of_col],\n",
    "    positions=[1.2, 1.4, 1.6, 1.8, 2],\n",
    "    labels=[\"1a\", \"1b\", \"1c\", \"2a\", \"2b\"],\n",
    ")\n",
    "ax3 = ax.boxplot(\n",
    "    noise_df.query(\n",
    "        \"combination=='src/main/rl/models/scenario2/training_04_06/scenario2_ActionSpaceOption3Wrapper_ObservationOption5Wrapper_None_RewardOption2Wrapper_PPO_training_04_06'\"\n",
    "    )[order_of_col],\n",
    "    positions=[2.4, 2.6, 2.8, 3, 3.2],\n",
    "    labels=[\"1a\", \"1b\", \"1c\", \"2a\", \"2b\"],\n",
    ")\n",
    "ax4 = ax.boxplot(\n",
    "    noise_df.query(\n",
    "        \"combination=='src/main/rl/models/scenario2/training_04_06/scenario2_ActionSpaceOption3Wrapper_ObservationOption4Wrapper_NPPAutomationWrapper_RewardOption2Wrapper_PPO_training_04_06'\"\n",
    "    )[order_of_col],\n",
    "    positions=[3.6, 3.8, 4, 4.2, 4.4],\n",
    "    labels=[\"1a\", \"1b\", \"1c\", \"2a\", \"2b\"],\n",
    ")\n",
    "ax5 = ax.boxplot(\n",
    "    noise_df.query(\n",
    "        \"combination=='src/main/rl/models/scenario3/training_04_06/scenario3_ActionSpaceOption3Wrapper_ObservationOption5Wrapper_None_RewardOption2Wrapper_PPO_training_04_06'\"\n",
    "    )[order_of_col],\n",
    "    positions=[4.8, 5, 5.2, 5.4, 5.6],\n",
    "    labels=[\"1a\", \"1b\", \"1c\", \"2a\", \"2b\"],\n",
    ")\n",
    "ax6 = ax.boxplot(\n",
    "    noise_df.query(\n",
    "        \"combination=='src/main/rl/models/scenario3/training_04_06/scenario3_ActionSpaceOption3Wrapper_ObservationOption3Wrapper_NPPAutomationWrapper_RewardOption2Wrapper_PPO_training_04_06'\"\n",
    "    )[order_of_col],\n",
    "    positions=[6, 6.2, 6.4, 6.6, 6.8],\n",
    "    labels=[\"1a\", \"1b\", \"1c\", \"2a\", \"2b\"],\n",
    ")\n",
    "set_box_color(ax1, color_mapping[\"blue\"], 0)\n",
    "set_box_color(ax1, color_mapping[\"blue\"], 1)\n",
    "set_box_color(ax2, color_mapping[\"red\"], 0)\n",
    "set_box_color(ax2, color_mapping[\"red\"], 1)\n",
    "set_box_color(ax3, color_mapping[\"grey\"], 0)\n",
    "set_box_color(ax3, color_mapping[\"grey\"], 1)\n",
    "set_box_color(ax4, color_mapping[\"yellow\"], 0)\n",
    "set_box_color(ax4, color_mapping[\"yellow\"], 1)\n",
    "set_box_color(ax5, color_mapping[\"brown\"], 0)\n",
    "set_box_color(ax5, color_mapping[\"brown\"], 1)\n",
    "set_box_color(ax6, color_mapping[\"turquoise\"], 0)\n",
    "set_box_color(ax6, color_mapping[\"turquoise\"], 1)\n",
    "ax.set_xlabel(\"Rauschoptionen\")\n",
    "ax.set_ylabel(\"Absolvierte Zeitschritte\")\n",
    "plt.plot([], c=color_mapping[\"blue\"], label=\"Kombination 1\")\n",
    "plt.plot([], c=color_mapping[\"red\"], label=\"Kombination 2\")\n",
    "plt.plot([], c=color_mapping[\"grey\"], label=\"Kombination 3\")\n",
    "plt.plot([], c=color_mapping[\"yellow\"], label=\"Kombination 4\")\n",
    "plt.plot([], c=color_mapping[\"brown\"], label=\"Kombination 5\")\n",
    "plt.plot([], c=color_mapping[\"turquoise\"], label=\"Kombination 6\")\n",
    "plt.legend(loc=[0.2, 0.1])\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout = True\n",
    "fig.savefig(\n",
    "    f\"src/main/rl/evaluation/plot_results/phase3_noise_by_combi.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_df.eq(250).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_agg = (\n",
    "    df[\n",
    "        [\n",
    "            \"DelayNoiseWrapperOption2_timesteps\",\n",
    "            \"DelayNoiseWrapperOption1_timesteps\",\n",
    "            \"ObservationVariesNoiseWrapper1_timesteps\",\n",
    "            \"ObservationVariesNoiseWrapper1_timesteps\",\n",
    "            \"ObservationVariesNegativeNoiseWrapper_timesteps\",\n",
    "            \"combination\",\n",
    "        ]\n",
    "    ]\n",
    "    .set_index(\"combination\")\n",
    "    .eq(250)\n",
    "    .groupby(\"combination\")\n",
    "    .sum()\n",
    "    .transpose()\n",
    "    .sum()\n",
    "    .transpose()\n",
    "    .to_latex()\n",
    ")\n",
    "print(noise_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check combination 1 model 3 for noise and why it fails in detail (plotting)\n",
    "import pandas as pd\n",
    "from src.main.rl.utils.parser import parse_wrapper\n",
    "from src.main.rl.utils.combined_parser import parse_information_from_path\n",
    "from src.main.rl.evaluation.eval import evaluate\n",
    "from src.main.rl.utils.utils import WrapperMaker\n",
    "from src.main.rl.evaluation.wrapper.noise.obs_varies_wrapper import (\n",
    "    ObservationVariesNegativeNoiseWrapper,\n",
    "    ObservationVariesPositiveNoiseWrapper,\n",
    ")\n",
    "from src.main.rl.evaluation.plots.phase3_plots import (\n",
    "    plot_observations,\n",
    "    plot_actions_taken,\n",
    ")\n",
    "from src.main.rl.evaluation.eval import evaluate, evaluate_terminal_state_obs\n",
    "\n",
    "path = \"src/main/rl/models/scenario1/training_04_06/scenario1_ActionSpaceOption3Wrapper_ObservationOption4Wrapper_None_RewardOption2Wrapper_TD3_training_04_06_3\"\n",
    "\n",
    "path_to_overhand = path + \"/best_model.zip\"\n",
    "\n",
    "action_wrapper, automation_wrapper, obs_wrapper, reward_wrapper = parse_wrapper(path)\n",
    "scenario, alg, wrapper_maker = parse_information_from_path(path)\n",
    "wrapper_maker = WrapperMaker(\n",
    "    action_wrapper,\n",
    "    automation_wrapper,\n",
    "    obs_wrapper,\n",
    "    reward_wrapper,\n",
    "    None,\n",
    "    ObservationVariesNegativeNoiseWrapper,\n",
    ")\n",
    "(\n",
    "    cum_reward,\n",
    "    criticality_score,\n",
    "    total_timesteps,\n",
    "    actions_taken,\n",
    "    obs_taken,\n",
    "    info,\n",
    ") = evaluate(scenario, path_to_overhand, alg, wrapper_maker, episode_length=5)\n",
    "cum_reward2, obs_taken2, info2 = evaluate_terminal_state_obs(\n",
    "    scenario, path_to_overhand, alg, wrapper_maker, episode_length=5\n",
    ")\n",
    "\n",
    "assert cum_reward2 == cum_reward\n",
    "y_axis_scale_obs = [[0, 1000], [0, 4000], [0, 550], [0, 8000], [0, 180], [0, 32]]\n",
    "\n",
    "fig = plot_observations(obs_taken2, y_axis_scale_obs)\n",
    "fig1 = plot_observations(obs_taken, y_axis_scale_obs)\n",
    "fig2 = plot_actions_taken(actions_taken, \"scenario1\")\n",
    "fig2[0].savefig(\n",
    "    f\"src/main/rl/evaluation/plot_results/phase3_noise_combi_1_action_not_working.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    ")\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_maker = WrapperMaker(\n",
    "    action_wrapper, automation_wrapper, obs_wrapper, reward_wrapper, None, None\n",
    ")\n",
    "(\n",
    "    cum_reward,\n",
    "    criticality_score,\n",
    "    total_timesteps,\n",
    "    actions_taken,\n",
    "    obs_taken,\n",
    "    info,\n",
    ") = evaluate(scenario, path_to_overhand, alg, wrapper_maker, episode_length=2)\n",
    "y_axis_scale_obs = [[0, 1000], [0, 4000], [0, 550], [0, 8000], [0, 180], [0, 32]]\n",
    "fig = plot_observations(obs_taken, y_axis_scale_obs)\n",
    "fig2 = plot_actions_taken(actions_taken, \"scenario1\")\n",
    "fig2[0].savefig(\n",
    "    f\"src/main/rl/evaluation/plot_results/phase3_noise_combi_1_action_not_working_comp_with_working.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    ")\n",
    "print(actions_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting State\n",
    "cols = [col for col in df.columns if \"starting\" in col and \"criticality\" not in col]\n",
    "df[cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"create_starting_state_option1_timesteps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check combination differen models (see paths) for noise and why they fail in detail\n",
    "import pandas as pd\n",
    "from src.main.rl.utils.parser import parse_wrapper\n",
    "from src.main.rl.utils.combined_parser import parse_information_from_path\n",
    "from src.main.rl.evaluation.eval import evaluate\n",
    "from src.main.rl.utils.utils import WrapperMaker\n",
    "from src.main.rl.evaluation.wrapper.noise.obs_varies_wrapper import (\n",
    "    ObservationVariesNegativeNoiseWrapper,\n",
    "    ObservationVariesPositiveNoiseWrapper,\n",
    ")\n",
    "from src.main.rl.evaluation.plots.phase3_plots import (\n",
    "    plot_observations,\n",
    "    plot_actions_taken,\n",
    ")\n",
    "from src.main.rl.evaluation.eval import evaluate, evaluate_terminal_state_obs\n",
    "from src.main.rl.utils.constants import (\n",
    "    ALL_OBS_NOISE_WRAPPERS,\n",
    "    ALL_DELAY_NOISE_WRAPPERS,\n",
    "    STARTING_STATE_OPTION1,\n",
    "    STARTING_STATE_OPTION2,\n",
    "    STARTING_STATE_OPTION3,\n",
    ")\n",
    "\n",
    "path = \"src/main/rl/models/scenario1/training_04_06/scenario1_ActionSpaceOption3Wrapper_ObservationOption4Wrapper_None_RewardOption2Wrapper_TD3_training_04_06_2\"\n",
    "# path = \"src/main/rl/models/scenario1/training_04_06/scenario1_ActionSpaceOption3Wrapper_ObservationOption4Wrapper_None_RewardOption2Wrapper_TD3_training_04_06_4\"\n",
    "# path = \"src/main/rl/models/scenario1/training_04_06/scenario1_ActionSpaceOption3Wrapper_ObservationOption4Wrapper_None_RewardOption2Wrapper_TD3_training_04_06_8\"\n",
    "# path =  \"src/main/rl/models/scenario3/training_04_06/scenario3_ActionSpaceOption3Wrapper_ObservationOption3Wrapper_NPPAutomationWrapper_RewardOption2Wrapper_PPO_training_04_06_5\"\n",
    "path_to_overhand = path + \"/best_model.zip\"\n",
    "\n",
    "action_wrapper, automation_wrapper, obs_wrapper, reward_wrapper = parse_wrapper(path)\n",
    "scenario, alg, wrapper_maker = parse_information_from_path(path)\n",
    "wrapper_maker = WrapperMaker(\n",
    "    action_wrapper, automation_wrapper, obs_wrapper, reward_wrapper, None, None\n",
    ")\n",
    "(\n",
    "    cum_reward,\n",
    "    criticality_score,\n",
    "    total_timesteps,\n",
    "    actions_taken,\n",
    "    obs_taken,\n",
    "    info,\n",
    ") = evaluate(\n",
    "    scenario,\n",
    "    path_to_overhand,\n",
    "    alg,\n",
    "    wrapper_maker,\n",
    "    episode_length=250,\n",
    "    starting_state=STARTING_STATE_OPTION1[0](),\n",
    ")\n",
    "cum_reward2, obs_taken2, info2 = evaluate_terminal_state_obs(\n",
    "    scenario,\n",
    "    path_to_overhand,\n",
    "    alg,\n",
    "    wrapper_maker,\n",
    "    episode_length=250,\n",
    "    starting_state=STARTING_STATE_OPTION1[0](),\n",
    ")\n",
    "print(info2)\n",
    "assert cum_reward2 == cum_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\n",
    "    [\n",
    "        \"create_starting_state_option2a_timesteps\",\n",
    "        \"create_starting_state_option2b_timesteps\",\n",
    "        \"create_starting_state_option2c_timesteps\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plots for starting state per option and per combination using timesteps\n",
    "def set_box_color(bp, color, idx):\n",
    "    plt.setp(bp[\"boxes\"], color=color)\n",
    "    plt.setp(bp[\"whiskers\"], color=color)\n",
    "    plt.setp(bp[\"caps\"], color=color)\n",
    "    plt.setp(bp[\"medians\"], color=color)\n",
    "    plt.setp(bp[\"caps\"], color=color)\n",
    "    plt.setp(bp[\"fliers\"], color=color)\n",
    "\n",
    "\n",
    "starting_state2 = df[\n",
    "    [\n",
    "        \"combination\",\n",
    "        \"create_starting_state_option2a_timesteps\",\n",
    "        \"create_starting_state_option2b_timesteps\",\n",
    "        \"create_starting_state_option2c_timesteps\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "order_of_col = [\n",
    "    \"create_starting_state_option2a_timesteps\",\n",
    "    \"create_starting_state_option2b_timesteps\",\n",
    "    \"create_starting_state_option2c_timesteps\",\n",
    "]\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "fig.set_figwidth(8)\n",
    "ax1 = ax.boxplot(\n",
    "    starting_state2.query(\n",
    "        \"combination=='src/main/rl/models/scenario1/training_04_06/scenario1_ActionSpaceOption3Wrapper_ObservationOption4Wrapper_None_RewardOption2Wrapper_TD3_training_04_06'\"\n",
    "    )[order_of_col],\n",
    "    positions=[\n",
    "        0,\n",
    "        0.2,\n",
    "        0.4,\n",
    "    ],\n",
    "    labels=[\n",
    "        \"2a\",\n",
    "        \"2b\",\n",
    "        \"2c\",\n",
    "    ],\n",
    ")\n",
    "ax2 = ax.boxplot(\n",
    "    starting_state2.query(\n",
    "        \"combination=='src/main/rl/models/scenario1/training_04_06/scenario1_ActionSpaceOption3Wrapper_ObservationOption5Wrapper_NPPAutomationWrapper_RewardOption2Wrapper_SAC_training_04_06'\"\n",
    "    )[order_of_col],\n",
    "    positions=[\n",
    "        0.9,\n",
    "        1.1,\n",
    "        1.3,\n",
    "    ],\n",
    "    labels=[\n",
    "        \"2a\",\n",
    "        \"2b\",\n",
    "        \"2c\",\n",
    "    ],\n",
    ")\n",
    "ax3 = ax.boxplot(\n",
    "    starting_state2.query(\n",
    "        \"combination=='src/main/rl/models/scenario2/training_04_06/scenario2_ActionSpaceOption3Wrapper_ObservationOption5Wrapper_None_RewardOption2Wrapper_PPO_training_04_06'\"\n",
    "    )[order_of_col],\n",
    "    positions=[1.8, 2, 2.2],\n",
    "    labels=[\n",
    "        \"2a\",\n",
    "        \"2b\",\n",
    "        \"2c\",\n",
    "    ],\n",
    ")\n",
    "ax4 = ax.boxplot(\n",
    "    starting_state2.query(\n",
    "        \"combination=='src/main/rl/models/scenario2/training_04_06/scenario2_ActionSpaceOption3Wrapper_ObservationOption4Wrapper_NPPAutomationWrapper_RewardOption2Wrapper_PPO_training_04_06'\"\n",
    "    )[order_of_col],\n",
    "    positions=[2.7, 2.9, 3.1],\n",
    "    labels=[\n",
    "        \"2a\",\n",
    "        \"2b\",\n",
    "        \"2c\",\n",
    "    ],\n",
    ")\n",
    "ax5 = ax.boxplot(\n",
    "    starting_state2.query(\n",
    "        \"combination=='src/main/rl/models/scenario3/training_04_06/scenario3_ActionSpaceOption3Wrapper_ObservationOption5Wrapper_None_RewardOption2Wrapper_PPO_training_04_06'\"\n",
    "    )[order_of_col],\n",
    "    positions=[3.6, 3.8, 4],\n",
    "    labels=[\n",
    "        \"2a\",\n",
    "        \"2b\",\n",
    "        \"2c\",\n",
    "    ],\n",
    ")\n",
    "ax6 = ax.boxplot(\n",
    "    starting_state2.query(\n",
    "        \"combination=='src/main/rl/models/scenario3/training_04_06/scenario3_ActionSpaceOption3Wrapper_ObservationOption3Wrapper_NPPAutomationWrapper_RewardOption2Wrapper_PPO_training_04_06'\"\n",
    "    )[order_of_col],\n",
    "    positions=[4.5, 4.7, 4.9],\n",
    "    labels=[\n",
    "        \"2a\",\n",
    "        \"2b\",\n",
    "        \"2c\",\n",
    "    ],\n",
    ")\n",
    "set_box_color(ax1, color_mapping[\"blue\"], 0)\n",
    "set_box_color(ax1, color_mapping[\"blue\"], 1)\n",
    "set_box_color(ax2, color_mapping[\"red\"], 0)\n",
    "set_box_color(ax2, color_mapping[\"red\"], 1)\n",
    "set_box_color(ax3, color_mapping[\"grey\"], 0)\n",
    "set_box_color(ax3, color_mapping[\"grey\"], 1)\n",
    "set_box_color(ax4, color_mapping[\"yellow\"], 0)\n",
    "set_box_color(ax4, color_mapping[\"yellow\"], 1)\n",
    "set_box_color(ax5, color_mapping[\"brown\"], 0)\n",
    "set_box_color(ax5, color_mapping[\"brown\"], 1)\n",
    "set_box_color(ax6, color_mapping[\"turquoise\"], 0)\n",
    "set_box_color(ax6, color_mapping[\"turquoise\"], 1)\n",
    "ax.set_xlabel(\"StartzustÃ¤nde\")\n",
    "ax.set_ylabel(\"Absolvierte Zeitschritte\")\n",
    "plt.plot([], c=color_mapping[\"blue\"], label=\"Kombination 1\")\n",
    "plt.plot([], c=color_mapping[\"red\"], label=\"Kombination 2\")\n",
    "plt.plot([], c=color_mapping[\"grey\"], label=\"Kombination 3\")\n",
    "plt.plot([], c=color_mapping[\"yellow\"], label=\"Kombination 4\")\n",
    "plt.plot([], c=color_mapping[\"brown\"], label=\"Kombination 5\")\n",
    "plt.plot([], c=color_mapping[\"turquoise\"], label=\"Kombination 6\")\n",
    "plt.legend(loc=[0.35, 0.5])\n",
    "plt.show()\n",
    "fig.savefig(\n",
    "    f\"src/main/rl/evaluation/plot_results/phase3_starting_state_2_by_combi.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state2.eq(250).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state2.query(\n",
    "        \"combination=='src/main/rl/models/scenario3/training_04_06/scenario3_ActionSpaceOption3Wrapper_ObservationOption3Wrapper_NPPAutomationWrapper_RewardOption2Wrapper_PPO_training_04_06'\"\n",
    "    ).eq(250).sum()\n",
    "starting_state2.query(\n",
    "        \"combination=='src/main/rl/models/scenario1/training_04_06/scenario1_ActionSpaceOption3Wrapper_ObservationOption4Wrapper_None_RewardOption2Wrapper_TD3_training_04_06'\"\n",
    "    ).eq(250).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"create_starting_state_option1_timesteps\"].eq(250).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state3 = df[\n",
    "    [\n",
    "        \"obs_wrapper\",\n",
    "        \"combination\",\n",
    "        \"automation_wrapper\",\n",
    "        \"scenario\",\n",
    "        \"create_starting_state_option3_timesteps\",\n",
    "    ]\n",
    "]\n",
    "starting_state3.groupby(\"combination\").agg(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state3.eq(250).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statistics_per_combination[\n",
    "    [\n",
    "        \"create_starting_state_option1_timesteps_mean\",\n",
    "        \"create_starting_state_option2a_timesteps_mean\",\n",
    "        \"create_starting_state_option2b_timesteps_mean\",\n",
    "        \"create_starting_state_option2c_timesteps_mean\",\n",
    "        \"create_starting_state_option3_timesteps_mean\",\n",
    "    ]\n",
    "].agg([\"mean\", \"max\", \"min\", \"std\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    df_statistics_per_combination[\n",
    "        [\n",
    "            \"create_starting_state_option1_timesteps_mean\",\n",
    "            \"create_starting_state_option2a_timesteps_mean\",\n",
    "            \"create_starting_state_option2b_timesteps_mean\",\n",
    "            \"create_starting_state_option2c_timesteps_mean\",\n",
    "            \"create_starting_state_option3_timesteps_mean\",\n",
    "        ]\n",
    "    ]\n",
    "    .agg([\"mean\", \"max\", \"min\", \"std\"], axis=1)\n",
    "    .droplevel(1)\n",
    "    .round(2)\n",
    "    .transpose()\n",
    "    .to_latex()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state_2 = (\n",
    "    df[\n",
    "        [\n",
    "            \"create_starting_state_option1_timesteps\",\n",
    "            \"create_starting_state_option2a_timesteps\",\n",
    "            \"create_starting_state_option2b_timesteps\",\n",
    "            \"create_starting_state_option2c_timesteps\",\n",
    "            \"create_starting_state_option3_timesteps\",\n",
    "            \"combination\",\n",
    "        ]\n",
    "    ]\n",
    "    .set_index(\"combination\")\n",
    "    .eq(250)\n",
    "    .groupby(\"combination\")\n",
    "    .sum()\n",
    "    .transpose()\n",
    "    .sum()\n",
    "    .transpose()\n",
    "    .to_latex()\n",
    ")\n",
    "\n",
    "print(starting_state_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state_total = df[\n",
    "    [\n",
    "        \"obs_wrapper\",\n",
    "        \"automation_wrapper\",\n",
    "        \"scenario\",\n",
    "        \"create_starting_state_option1_timesteps\",\n",
    "        \"create_starting_state_option2a_timesteps\",\n",
    "        \"create_starting_state_option2b_timesteps\",\n",
    "        \"create_starting_state_option2c_timesteps\",\n",
    "        \"create_starting_state_option3_timesteps\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state_total.eq(250).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of model-experiment combinations that sucessfully executed all timesteps during the specific experiment.\n",
    "# In total there were 660 such combinations\n",
    "(\n",
    "    df[\n",
    "        [\n",
    "            \"DelayNoiseWrapperOption2_timesteps\",\n",
    "            \"DelayNoiseWrapperOption1_timesteps\",\n",
    "            \"ObservationVariesNoiseWrapper1_timesteps\",\n",
    "            \"ObservationVariesNoiseWrapper1_timesteps\",\n",
    "            \"ObservationVariesNegativeNoiseWrapper_timesteps\",\n",
    "            \"create_starting_state_option1_timesteps\",\n",
    "            \"create_starting_state_option2a_timesteps\",\n",
    "            \"create_starting_state_option2b_timesteps\",\n",
    "            \"create_starting_state_option2c_timesteps\",\n",
    "            \"create_starting_state_option3_timesteps\",\n",
    "        ]\n",
    "    ]\n",
    "    .eq(250)\n",
    "    .sum()\n",
    "    .sum()\n",
    "    + df[\"episode_length_1000_timesteps\"].eq(1000).sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of model-experiment combinations per combination that sucessfully executed all timesteps during the specific experiment.\n",
    "# In total there were 110 such combinations per combination\n",
    "# Carefull: The order of the combination might not be the same as in the thesis!\n",
    "\n",
    "def get_counts_by_group(df, max_timesteps):\n",
    "    return df.eq(max_timesteps).sum()\n",
    "\n",
    "counts_noise=(\n",
    "    df[\n",
    "        [   \"combination\",\n",
    "            \"DelayNoiseWrapperOption2_timesteps\",\n",
    "            \"DelayNoiseWrapperOption1_timesteps\",\n",
    "            \"ObservationVariesNoiseWrapper1_timesteps\",\n",
    "            \"ObservationVariesNoiseWrapper1_timesteps\",\n",
    "            \"ObservationVariesNegativeNoiseWrapper_timesteps\",\n",
    "            \n",
    "        ]\n",
    "    ].groupby(\"combination\").apply(get_counts_by_group,(250))).sum(axis=1)\n",
    "counts_starting_state=(\n",
    "    df[\n",
    "        [   \"combination\",\n",
    "            \"create_starting_state_option1_timesteps\",\n",
    "            \"create_starting_state_option2a_timesteps\",\n",
    "            \"create_starting_state_option2b_timesteps\",\n",
    "            \"create_starting_state_option2c_timesteps\",\n",
    "            \"create_starting_state_option3_timesteps\",\n",
    "            \n",
    "        ]\n",
    "    ].groupby(\"combination\").apply(get_counts_by_group,(250))).sum(axis=1)\n",
    "counts_1000=df[[\"combination\",\"episode_length_1000_timesteps\"]].groupby(\"combination\").apply(get_counts_by_group,(1000))[\"episode_length_1000_timesteps\"]\n",
    "\n",
    "print(\"Counts over all modification experiments with noise:\")\n",
    "print(counts_noise )\n",
    "print(\"Counts over all modification experiments with starting states:\")\n",
    "print(counts_starting_state)\n",
    "print(\"Counts over all modification experiment with 100 length:\")\n",
    "print(counts_1000)\n",
    "\n",
    "print(\"Counts over all modification experiments:\")\n",
    "print(counts_noise + counts_starting_state+counts_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting state comparison grouped by different categories e.g. scenario by  timesteps\n",
    "starting_state_total.loc[\n",
    "    starting_state_total[\"automation_wrapper\"].isna(), \"automation_wrapper\"\n",
    "] = \"NaN\"\n",
    "starting_state_total.groupby(\"scenario\").agg([\"mean\"], axis=0).round(2).agg(\n",
    "    \"mean\", axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state_total.loc[\n",
    "    starting_state_total[\"automation_wrapper\"].isna(), \"automation_wrapper\"\n",
    "] = \"NaN\"\n",
    "starting_state_total.groupby(\"automation_wrapper\").agg([\"mean\"], axis=0).round(2).agg(\n",
    "    \"mean\", axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test (Welch) -> non normality therefore ttest can not be used / results have to be interpreted carefully\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "wo_NPP = starting_state_total[starting_state_total[\"automation_wrapper\"] == \"NaN\"].drop(\n",
    "    columns=[\"obs_wrapper\", \"scenario\", \"automation_wrapper\"]\n",
    ")\n",
    "wo_NPP = wo_NPP.stack().reset_index(drop=True)\n",
    "w_NPP = starting_state_total[\n",
    "    starting_state_total[\"automation_wrapper\"] == \"NPPAutomationWrapper\"\n",
    "].drop(columns=[\"obs_wrapper\", \"scenario\", \"automation_wrapper\"])\n",
    "w_NPP = w_NPP.stack().reset_index(drop=True)\n",
    "\n",
    "ttest_ind(wo_NPP, w_NPP, equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_NPP = starting_state_total[starting_state_total[\"automation_wrapper\"] == \"NaN\"].drop(\n",
    "    columns=[\"obs_wrapper\", \"scenario\", \"automation_wrapper\"]\n",
    ")[\"create_starting_state_option3_timesteps\"]\n",
    "w_NPP = starting_state_total[\n",
    "    starting_state_total[\"automation_wrapper\"] == \"NPPAutomationWrapper\"\n",
    "].drop(columns=[\"obs_wrapper\", \"scenario\", \"automation_wrapper\"])[\n",
    "    \"create_starting_state_option3_timesteps\"\n",
    "]\n",
    "ttest_ind(wo_NPP, w_NPP, equal_var=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
